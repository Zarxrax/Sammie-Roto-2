defaults:
  - _self_
  - model: base
  - override hydra/job_logging: custom-no-rank.yaml

hydra:
  run:
    dir: ../output/${exp_id}/${dataset}
  output_subdir: ${now:%Y-%m-%d_%H-%M-%S}-hydra

amp: False
weights: pretrained_models/matanyone.pth # default (can be modified from outside)
output_dir: null # defaults to run_dir; specify this to override
flip_aug: False


# maximum shortest side of the input; -1 means no resizing
# With eval_vos.py, we usually just use the dataset's size (resizing done in dataloader)
# this parameter is added for the sole purpose for the GUI in the current codebase
# InferenceCore will downsize the input and restore the output to the original size if needed
# if you are using this code for some other project, you can also utilize this parameter
max_internal_size: -1

# these parameters, when set, override the dataset's default; useful for debugging
save_all: True
use_all_masks: False
use_long_term: False
mem_every: 5

# only relevant when long_term is not enabled
max_mem_frames: 5

# only relevant when long_term is enabled
long_term:
  count_usage: True
  max_mem_frames: 10
  min_mem_frames: 5
  num_prototypes: 128
  max_num_tokens: 10000
  buffer_tokens: 2000

top_k: 30
stagger_updates: 5
chunk_size: -1 # number of objects to process in parallel; -1 means unlimited
save_scores: False
save_aux: False
visualize: False

model:
    pixel_mean: [0.485, 0.456, 0.406]
    pixel_std: [0.229, 0.224, 0.225]

    pixel_dim: 256
    key_dim: 64
    value_dim: 256
    sensory_dim: 256
    embed_dim: 256

    pixel_encoder:
      type: resnet50
      ms_dims: [1024, 512, 256, 64, 3] # f16, f8, f4, f2, f1

    mask_encoder:
      type: resnet18
      final_dim: 256

    pixel_pe_scale: 32
    pixel_pe_temperature: 128

    object_transformer:
      embed_dim: ${model.embed_dim}
      ff_dim: 2048
      num_heads: 8
      num_blocks: 3
      num_queries: 16
      read_from_pixel:
        input_norm: False
        input_add_pe: False
        add_pe_to_qkv: [True, True, False]
      read_from_past:
        add_pe_to_qkv: [True, True, False]
      read_from_memory:
        add_pe_to_qkv: [True, True, False]
      read_from_query:
        add_pe_to_qkv: [True, True, False]
        output_norm: False
      query_self_attention:
        add_pe_to_qkv: [True, True, False]
      pixel_self_attention:
        add_pe_to_qkv: [True, True, False]

    object_summarizer:
      embed_dim: ${model.object_transformer.embed_dim}
      num_summaries: ${model.object_transformer.num_queries}
      add_pe: True

    aux_loss:
      sensory:
        enabled: True
        weight: 0.01
      query:
        enabled: True
        weight: 0.01

    mask_decoder:
      # first value must equal embed_dim
      up_dims: [256, 128, 128, 64, 16]